{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a521b6-4d28-4921-b2ec-2465e6f46ca5",
   "metadata": {},
   "source": [
    "# Домашнее задание № 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6073c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import skimage\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "# from PCV.geometry import homography\n",
    "# from PCV.localdescriptors import sift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30cf95-80f0-432d-9053-4cb9776992ee",
   "metadata": {},
   "source": [
    "# Поиск объектов в видеопотоке "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a5193-25dc-4df2-91b1-77860dac156f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Задача - придумать и реализовать алгоритм поиска (обнаружения без классификации) движущихся объектов.\n",
    "\n",
    "В качестве исходных данных приведена выборка с видеофайлами и аннотацией для каждого кадра файла. Аннотация задана в виде ограничивающих прямоугольников в формате ```(y1,x1,y2,x2)```, где\n",
    "- ```(x1,y1)``` - верхний левый угол прямоугольника;\n",
    "- ```(x2,y2)``` - нижний правый угол прямоугольника.\n",
    "\n",
    "Ссылка на данные – https://disk.yandex.ru/d/RdjMDoQQO8Ngcw\n",
    "\n",
    "В качестве обучающей можно брать любые видеофайлы. При этом должны быть отдельно выбраны тестовые данные, которые не будут использованы в создании решения. \n",
    "\n",
    "Видеофайл с результатами работы алгоритма должен быть прикреплен вместе с решением. Пример фрагмента видеофайла с результатом поиска объектов приведен ниже.\n",
    "\n",
    "Исходный код может быть в формате ```.py``` или ```.ipynb```.\n",
    "\n",
    "![annotation](annot_example.gif \"annotation\")\n",
    "\n",
    "## Требования к результату\n",
    "- поиск должен находить геометрические место объекта на видеоизображении. Геометрическое место задано ограничивающим прямоугольником (bounding box);\n",
    "- продолжительность решения для любого одного видеофайла не должна превышать 10 минут;\n",
    "- должна быть приведена оценка точности решения;\n",
    "- привести демонстрацию результатов требется на одном из тестовых видеофайлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b465e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательная функция\n",
    "def plot_one_image(src_image, is_gray=False):\n",
    "    \"\"\"\n",
    "    Отрисовать с помощью plt исходное изображение.\n",
    "    \n",
    "    :param src_image: np.ndarray: исходное изображение\n",
    "    :param is_gray: bool: флаг для отображения ЧБ изображений\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, m_axs = plt.subplots(1, 1, figsize=(6*2, 4*2), constrained_layout=True)\n",
    "    ax1 = m_axs\n",
    "\n",
    "    cmap = 'gray' if is_gray else None\n",
    "    ax1.set_title('Исходное изображение')\n",
    "    ax1.imshow(src_image, cmap=cmap)\n",
    "    ax1.set_xticks([]), ax1.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "20f92554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    def __init__(self):\n",
    "        self.points = []\n",
    "    \n",
    "    def add_point(self, x, y):\n",
    "        self.points.append(np.array([x,y]))\n",
    "    \n",
    "    def get_last(self):\n",
    "        return self.points[-1]\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.points)\n",
    "    \n",
    "    def continue_trajectory(self, model):\n",
    "        self.points.append(model.predict([self.points[-1]])[0])\n",
    "#         if len(self.points) > 1:\n",
    "#             self.points.append(model.predict([self.points[-1]])[0] + self.points[-1]-self.points[-2])\n",
    "#         else:\n",
    "#             self.points.append(model.predict([self.points[-1]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f595d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectories:\n",
    "    def __init__(self):\n",
    "        self.trajectories = []\n",
    "    \n",
    "    def add_trajectories(self, points):\n",
    "        last_points = np.array([trajectory.get_last() for trajectory in self.trajectories])\n",
    "        for point in points:\n",
    "            point = point[0]\n",
    "            if len(last_points) == 0 or np.min(np.abs(last_points[:,0] - point[0])) + \\\n",
    "                                        np.min(np.abs(last_points[:,1] - point[1])) > 10:\n",
    "                self.trajectories.append(Trajectory())\n",
    "                self.trajectories[-1].add_point(point[0], point[1])\n",
    "        \n",
    "    def get_last_points(self, model=None, w=None, h=None):\n",
    "        if not model:\n",
    "            return np.array([np.array([trajectory.get_last()]) for trajectory in self.trajectories])\n",
    "        \n",
    "        last_points = []\n",
    "        bad_ind = []\n",
    "        for i, trajectory in enumerate(self.trajectories):\n",
    "            if trajectory.len() == 1:\n",
    "                last_points.append([trajectory.get_last()])\n",
    "            else:\n",
    "#                 print(trajectory.get_last())\n",
    "                new_point = model.predict([trajectory.get_last()])[0]\n",
    "                if new_point[0] < w and new_point[0] > 0 and new_point[1] < h and new_point[1] > 0:\n",
    "                    last_points.append([new_point.astype(np.uint8)])\n",
    "                else:\n",
    "                    bad_ind.append(i)\n",
    "                    \n",
    "        for i in reversed(bad_ind):\n",
    "            del self.trajectories[i]\n",
    "        return np.array(last_points)\n",
    "            \n",
    "    \n",
    "    def continue_trajectories(self, new_points, old_points, model):\n",
    "        i = 0\n",
    "        for trajectory in self.trajectories:\n",
    "            if np.sum(np.abs(model.predict([trajectory.get_last()])[0] - old_points[i])) < 3:\n",
    "                trajectory.add_point(new_points[i][0], new_points[i][1])\n",
    "                i += 1\n",
    "            else:\n",
    "                trajectory.continue_trajectory(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9407eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(save_dir, size, img_format='jpg', vido_format='avi'):\n",
    "    out_name = Path(save_dir).parts[-1]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(Path(save_dir) / Path(f'{out_name}.{vido_format}')),\n",
    "                          fourcc, 20, tuple(size.astype(int)))\n",
    "\n",
    "    for fname in tqdm(sorted(map(str, Path(save_dir).glob(f'*.{img_format}')))):\n",
    "        imag = skimage.io.imread(fname)\n",
    "        imag = cv2.cvtColor(imag, cv2.COLOR_RGB2BGR)\n",
    "        out.write(imag)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a0d92a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(num=-1,path='./tmp/', sleep=0):\n",
    "    cap = get_video(num if num >=0 else 0, path=path)\n",
    "    get_video_details(cap)\n",
    "    cv2.startWindowThread()\n",
    "    while (cap.isOpened()):\n",
    "        is_ok, frame = cap.read()\n",
    "        if not is_ok:\n",
    "            break\n",
    "            \n",
    "        if sleep > 0:\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        cv2.imshow(\"sparse optical flow\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7177b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_points(image):\n",
    "    \"\"\"\n",
    "    Функция для поиска особых точек и получения их дескрипторов \n",
    "    \"\"\"\n",
    "    \n",
    "    hyp_params = dict(\n",
    "        nfeatures = 250,\n",
    "        nOctaveLayers = 10,\n",
    "        contrastThreshold = 0.005,\n",
    "        edgeThreshold = 6,\n",
    "        sigma = 2.0)  # hyp params\n",
    "    detector = cv2.SIFT_create(**hyp_params)\n",
    "\n",
    "    keypoints, desc = detector.detectAndCompute(image.copy(), None)\n",
    "    return keypoints, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2f280442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyp_and_d(first, second):\n",
    "    FLANN_INDEX_KDTREE = 2\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    ratio_thresh = 0.55\n",
    "    keypoints1, desc1 = spec_points(first)\n",
    "    keypoints2, desc2 = spec_points(second)\n",
    "    \n",
    "    matches = flann.knnMatch(desc1, desc2, k=2)\n",
    "    \n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_thresh * n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    return keypoints1, keypoints2, good_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "3f6e815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_M(first, second):\n",
    "    h, w = first.shape\n",
    "    \n",
    "    \n",
    "    keypoints1, keypoints2, good_matches = get_keyp_and_d(first, second)\n",
    "    \n",
    "    pts1 = []\n",
    "    pts2 = []   \n",
    "    \n",
    "    for good_matche in good_matches:\n",
    "        pts1.append(keypoints1[good_matche.queryIdx].pt)\n",
    "        pts2.append(keypoints2[good_matche.trainIdx].pt)\n",
    "    \n",
    "    pts1 = np.array(pts1).astype(np.float32)\n",
    "    pts2 = np.array(pts2).astype(np.float32)\n",
    "    \n",
    "#     print('pts1', pts1)\n",
    "#     print('pts2', pts2)\n",
    "    \n",
    "    model = RANSACRegressor()\n",
    "    model.fit(pts1, pts2)\n",
    "\n",
    "    points1 = np.array([[0,0], [0,h], [w,0], [w,h]]).astype(np.float32)\n",
    "    points2 = model.predict(points1).astype(np.float32)\n",
    "    M = cv2.getPerspectiveTransform(points1, points2)\n",
    "    return M, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "20dcc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(cap, max_idx=-1):\n",
    "    cnt, w, h, fps = get_video_details(cap)\n",
    "    cnt = int(cnt)\n",
    "    color = (0, 255, 0)\n",
    "    feature_params = dict(maxCorners=1000, qualityLevel=0.02, minDistance=2, blockSize=7)\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "    is_ok, first_frame = cap.read()\n",
    "    if not is_ok:\n",
    "        print(\"ERR: Can't read\")\n",
    "        return\n",
    "    \n",
    "    prev_gray = cv2.cvtColor(first_frame.copy(), cv2.COLOR_BGR2GRAY)\n",
    "#     prev = cv2.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
    "    mask = np.zeros_like(first_frame)\n",
    "    idx = 0\n",
    "    trajectories = Trajectories()\n",
    "    for idx in tqdm(range(cnt-1)):\n",
    "        if not cap.isOpened():\n",
    "            break\n",
    "#         print(\"idx\", idx)\n",
    "        if max_idx > 0 and idx > max_idx:\n",
    "            break\n",
    "        is_ok, frame = cap.read()\n",
    "        if not is_ok:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame.copy(), cv2.COLOR_BGR2GRAY)\n",
    "        mask = np.zeros_like(frame)\n",
    "        \n",
    "#         print('prob:', np.sum((prev_gray - gray)**2))\n",
    "        M, model = get_M(prev_gray.copy(), gray.copy())\n",
    "        trans_prev_gray = cv2.warpPerspective(prev_gray.copy(), M, (prev_gray.shape[1], prev_gray.shape[0]))\n",
    "\n",
    "\n",
    "        prev = cv2.goodFeaturesToTrack(trans_prev_gray, mask=None, **feature_params)\n",
    "        trajectories.add_trajectories(prev)\n",
    "        prev = trajectories.get_last_points(model, w, h)\n",
    "        trans_gray = gray*(trans_prev_gray!=0).astype(np.uint8)\n",
    "        nextp, status, error = cv2.calcOpticalFlowPyrLK(trans_prev_gray, trans_gray, prev, None, **lk_params)\n",
    "        \n",
    "        good_old = prev[status == 1].astype(int)\n",
    "        good_new = nextp[status == 1].astype(int)\n",
    "\n",
    "        # Draws the optical flow tracks\n",
    "        new_points = []\n",
    "        old_points = []\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            mask = cv2.line(mask, (a, b), (c, d), color, 2)\n",
    "            frame = cv2.circle(frame, (a, b), 3, color, -1)\n",
    "            new_points.append(np.array([a, b]))\n",
    "            old_points.append(np.array([c, d]))\n",
    "        trajectories.continue_trajectories(new_points, old_points, model)\n",
    "\n",
    "        output = cv2.add(frame, mask)\n",
    "        prev_gray = gray.copy()\n",
    "        # Updates previous good feature points\n",
    "        prev = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "        cv2.imwrite(f'tmp/frame_{str(idx).zfill(4)}.jpg', output)\n",
    "\n",
    "    # The following frees up resources and closes all windows\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "bf1bc522",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clip_01.mov', 'Clip_02.mov', 'Clip_03.mov', 'Clip_04.mov', 'Clip_05.mov', 'Clip_06.mov', 'Clip_07.mov', 'Clip_08.mov', 'Clip_09.mov', 'Clip_10.mov', 'Clip_11.mov', 'Clip_37.mov']\n",
      "./Videos/Videos/Clip_01.mov\n",
      "309.0 1920.0 1080.0 29.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae88a5374d74e6d9e9607194f3f919c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\lkpyramid.cpp:1260: error: (-215:Assertion failed) (npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0 in function 'cv::`anonymous-namespace'::SparsePyrLKOpticalFlowImpl::calc'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [266], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m cap \u001b[38;5;241m=\u001b[39m get_video(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mget_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcap\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [265], line 39\u001b[0m, in \u001b[0;36mget_trajectory\u001b[1;34m(cap, max_idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m prev \u001b[38;5;241m=\u001b[39m trajectories\u001b[38;5;241m.\u001b[39mget_last_points(model, w, h)\n\u001b[0;32m     38\u001b[0m trans_gray \u001b[38;5;241m=\u001b[39m gray\u001b[38;5;241m*\u001b[39m(trans_prev_gray\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m---> 39\u001b[0m nextp, status, error \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcalcOpticalFlowPyrLK(trans_prev_gray, trans_gray, prev, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlk_params)\n\u001b[0;32m     41\u001b[0m good_old \u001b[38;5;241m=\u001b[39m prev[status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     42\u001b[0m good_new \u001b[38;5;241m=\u001b[39m nextp[status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\lkpyramid.cpp:1260: error: (-215:Assertion failed) (npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0 in function 'cv::`anonymous-namespace'::SparsePyrLKOpticalFlowImpl::calc'\n"
     ]
    }
   ],
   "source": [
    "cap = get_video(0)\n",
    "get_trajectory(cap)\n",
    "# create_video('tmp', size=np.array((1920.0 , 1080.0)), vido_format='mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "33288558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc374e8158034277b35e000c1eb45c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_video('tmp', size=np.array((1920.0 , 1080.0)), vido_format='mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "281fbe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tmp.mp4']\n",
      "./tmp/tmp.mp4\n",
      "101.0 1920.0 1080.0 20.0\n"
     ]
    }
   ],
   "source": [
    "show_video(sleep=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "583b0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(cap):\n",
    "    cnt = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    print(cnt, w, h, fps)\n",
    "    return cnt, w, h, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be76a7f8-b6e7-4fcf-9799-c4a0b0d9f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(num, path='./Videos/Videos/'):\n",
    "    \"\"\"\n",
    "    Функция достающая видео\n",
    "    num - номер видео (если num == -1, то достаются все видео)\n",
    "    \n",
    "    \"\"\"\n",
    "    all_names = os.listdir(path)\n",
    "    names = []\n",
    "    for name in all_names:\n",
    "        if name.endswith(\".mov\") or name.endswith(\".mp4\"):\n",
    "            names.append(name)\n",
    "    print(names)\n",
    "    if num >= 0:\n",
    "        name_vid = names[num]\n",
    "        print(path+name_vid)\n",
    "        return cv2.VideoCapture(path+name_vid)\n",
    "    \n",
    "    videos = []\n",
    "    for name_vid in names:\n",
    "        videos.append(cv2.VideoCapture(path+name_vid))\n",
    "    return videos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef74c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clip_1.mov', 'Clip_10.mov', 'Clip_11.mov', 'Clip_2.mov', 'Clip_3.mov', 'Clip_37.mov', 'Clip_4.mov', 'Clip_5.mov', 'Clip_6.mov', 'Clip_7.mov', 'Clip_8.mov', 'Clip_9.mov']\n",
      "./Videos/Videos/Clip_1.mov\n",
      "309.0 1920.0 1080.0 29.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mshow_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Videos/Videos/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [4], line 11\u001b[0m, in \u001b[0;36mshow_video\u001b[1;34m(num, path, sleep)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sleep \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse optical flow\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "show_video(0, './Videos/Videos/', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6a5be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tmp.mp4']\n",
      "./tmp/tmp.mp4\n",
      "308.0 1920.0 1080.0 20.0\n"
     ]
    }
   ],
   "source": [
    "show_video(sleep=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4c50162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9609830a510c4c098de131866e0232d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tmp.mov']\n",
      "./tmp/tmp.mov\n",
      "308.0 1920.0 1080.0 20.0\n"
     ]
    }
   ],
   "source": [
    "create_video('tmp', size=np.array((1920.0 , 1080.0)), vido_format='mov')\n",
    "show_video(sleep=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070337f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
